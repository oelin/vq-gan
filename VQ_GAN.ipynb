{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f104f76-341f-42a0-9979-2af06187a368",
      "metadata": {
        "id": "0f104f76-341f-42a0-9979-2af06187a368"
      },
      "source": [
        "# VQ-GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1f14331-b424-44b3-8056-61d9f0562a87",
      "metadata": {
        "tags": [],
        "id": "e1f14331-b424-44b3-8056-61d9f0562a87"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3b7bae6b-9501-45fb-b039-97dbcd056d5b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T08:41:48.541759Z",
          "iopub.status.busy": "2023-12-05T08:41:48.541402Z",
          "iopub.status.idle": "2023-12-05T08:41:56.606703Z",
          "shell.execute_reply": "2023-12-05T08:41:56.605885Z",
          "shell.execute_reply.started": "2023-12-05T08:41:48.541732Z"
        },
        "tags": [],
        "id": "3b7bae6b-9501-45fb-b039-97dbcd056d5b"
      },
      "outputs": [],
      "source": [
        "!pip -q install lightning einops datasets tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b87c7f77-156a-4a92-8a97-db7cb3486ed0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T08:41:56.608533Z",
          "iopub.status.busy": "2023-12-05T08:41:56.608216Z",
          "iopub.status.idle": "2023-12-05T08:42:03.365142Z",
          "shell.execute_reply": "2023-12-05T08:42:03.364549Z",
          "shell.execute_reply.started": "2023-12-05T08:41:56.608506Z"
        },
        "tags": [],
        "id": "b87c7f77-156a-4a92-8a97-db7cb3486ed0"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.optim import Adam\n",
        "\n",
        "import lightning as L\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fb58969-8250-425d-8035-c20019fec278",
      "metadata": {
        "id": "3fb58969-8250-425d-8035-c20019fec278"
      },
      "source": [
        "## 2. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c40c1961-50b3-4acb-b8b4-91b06b2a1c83",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "c40c1961-50b3-4acb-b8b4-91b06b2a1c83"
      },
      "source": [
        "### 2.1. Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a90af9ed-ffef-4987-9481-c1fe74cad0c5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T10:39:03.313576Z",
          "iopub.status.busy": "2023-12-05T10:39:03.313162Z",
          "iopub.status.idle": "2023-12-05T10:39:03.317429Z",
          "shell.execute_reply": "2023-12-05T10:39:03.316543Z",
          "shell.execute_reply.started": "2023-12-05T10:39:03.313549Z"
        },
        "tags": [],
        "id": "a90af9ed-ffef-4987-9481-c1fe74cad0c5"
      },
      "outputs": [],
      "source": [
        "Normalization = lambda in_channels: nn.GroupNorm(\n",
        "    num_groups=32,\n",
        "    num_channels=in_channels,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d94fd90-3d4d-47bd-b741-e4f438a13743",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "6d94fd90-3d4d-47bd-b741-e4f438a13743"
      },
      "source": [
        "### 2.2. Convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "97cdd8c8-c5ea-41b9-82a6-f42cfbf2e36f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T10:40:13.924559Z",
          "iopub.status.busy": "2023-12-05T10:40:13.924165Z",
          "iopub.status.idle": "2023-12-05T10:40:13.928471Z",
          "shell.execute_reply": "2023-12-05T10:40:13.927593Z",
          "shell.execute_reply.started": "2023-12-05T10:40:13.924534Z"
        },
        "tags": [],
        "id": "97cdd8c8-c5ea-41b9-82a6-f42cfbf2e36f"
      },
      "outputs": [],
      "source": [
        "Convolution = lambda in_channels, out_channels: nn.Conv2d(\n",
        "    in_channels=in_channels,\n",
        "    out_channels=out_channels,\n",
        "    kernel_size=3,\n",
        "    stride=1,\n",
        "    padding=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d72bf03-252b-43a6-8d2d-9407f3fd3667",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "7d72bf03-252b-43a6-8d2d-9407f3fd3667"
      },
      "source": [
        "### 2.3. Pointwise Convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "439bc3d5-ed3f-4d04-b921-dbab68108c0c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T10:40:58.997082Z",
          "iopub.status.busy": "2023-12-05T10:40:58.996670Z",
          "iopub.status.idle": "2023-12-05T10:40:59.000820Z",
          "shell.execute_reply": "2023-12-05T10:40:59.000168Z",
          "shell.execute_reply.started": "2023-12-05T10:40:58.997057Z"
        },
        "tags": [],
        "id": "439bc3d5-ed3f-4d04-b921-dbab68108c0c"
      },
      "outputs": [],
      "source": [
        "PointwiseConvolution = lambda in_channels: nn.Conv2d(\n",
        "    in_channels=in_channels,\n",
        "    out_channels=in_channels,\n",
        "    kernel_size=1,\n",
        "    stride=1,\n",
        "    padding=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94333634-5c5a-466b-ba33-ebf64354225c",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "94333634-5c5a-466b-ba33-ebf64354225c"
      },
      "source": [
        "### 2.4. Downsample Convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b4c300f0-81d9-46ce-8dfd-f8d6a90d6ea0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T10:41:51.758942Z",
          "iopub.status.busy": "2023-12-05T10:41:51.758531Z",
          "iopub.status.idle": "2023-12-05T10:41:51.763015Z",
          "shell.execute_reply": "2023-12-05T10:41:51.761975Z",
          "shell.execute_reply.started": "2023-12-05T10:41:51.758913Z"
        },
        "tags": [],
        "id": "b4c300f0-81d9-46ce-8dfd-f8d6a90d6ea0"
      },
      "outputs": [],
      "source": [
        "DownsampleConvolution = lambda in_channels, out_channels: nn.Conv2d(\n",
        "    in_channels=in_channels,\n",
        "    out_channels=out_channels,\n",
        "    kernel_size=4,\n",
        "    stride=2,\n",
        "    padding=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d87ebc7a-43be-40dd-a3c9-d3d91f1556dc",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "d87ebc7a-43be-40dd-a3c9-d3d91f1556dc"
      },
      "source": [
        "### 2.5. Activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6f959e81-acf4-4b98-8850-205b1af8d0b5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T10:33:31.912446Z",
          "iopub.status.busy": "2023-12-05T10:33:31.912068Z",
          "iopub.status.idle": "2023-12-05T10:33:31.916787Z",
          "shell.execute_reply": "2023-12-05T10:33:31.915802Z",
          "shell.execute_reply.started": "2023-12-05T10:33:31.912418Z"
        },
        "tags": [],
        "id": "6f959e81-acf4-4b98-8850-205b1af8d0b5"
      },
      "outputs": [],
      "source": [
        "Activation = lambda: nn.LeakyReLU(0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b626d3be-6600-4cb8-965b-7ef8639d315f",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "b626d3be-6600-4cb8-965b-7ef8639d315f"
      },
      "source": [
        "### 2.6. ResNet Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "441ae635-762a-4066-99a0-b18b097b5b3c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T11:01:12.965679Z",
          "iopub.status.busy": "2023-12-05T11:01:12.964975Z",
          "iopub.status.idle": "2023-12-05T11:01:12.970425Z",
          "shell.execute_reply": "2023-12-05T11:01:12.969710Z",
          "shell.execute_reply.started": "2023-12-05T11:01:12.965647Z"
        },
        "tags": [],
        "id": "441ae635-762a-4066-99a0-b18b097b5b3c"
      },
      "outputs": [],
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "    \"\"\"ResNet Block.\"\"\"\n",
        "\n",
        "    def __init__(self, *, in_channels: int) -> None:\n",
        "        \"\"\"Initialize the module.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.sequential = nn.Sequential(\n",
        "            Normalization(in_channels=in_channels),\n",
        "            Activation(),\n",
        "            Convolution(in_channels=in_channels, out_channels=in_channels),\n",
        "            Normalization(in_channels=in_channels),\n",
        "            Activation(),\n",
        "            Convolution(in_channels=in_channels, out_channels=in_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "\n",
        "        return x + self.sequential(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cf96674-f440-450f-ba45-a91855cb234b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "0cf96674-f440-450f-ba45-a91855cb234b"
      },
      "source": [
        "### 2.7. Attention Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c6540eda-e084-4d94-abde-666396da13d7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T11:01:14.501788Z",
          "iopub.status.busy": "2023-12-05T11:01:14.501404Z",
          "iopub.status.idle": "2023-12-05T11:01:14.507908Z",
          "shell.execute_reply": "2023-12-05T11:01:14.507075Z",
          "shell.execute_reply.started": "2023-12-05T11:01:14.501762Z"
        },
        "tags": [],
        "id": "c6540eda-e084-4d94-abde-666396da13d7"
      },
      "outputs": [],
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"Attention block.\"\"\"\n",
        "\n",
        "    def __init__(self, *, in_channels: int) -> None:\n",
        "        \"\"\"Initialize the module.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.normalization = Normalization(in_channels=in_channels)\n",
        "        self.project_q = PointwiseConvolution(in_channels=in_channels)\n",
        "        self.project_k = PointwiseConvolution(in_channels=in_channels)\n",
        "        self.project_v = PointwiseConvolution(in_channels=in_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "\n",
        "        b, c, h, w = x.shape\n",
        "        z = self.normalization(x)\n",
        "\n",
        "        q = self.project_q(z)\n",
        "        k = self.project_k(z)\n",
        "        v = self.project_v(z)\n",
        "\n",
        "        q = rearrange(q, 'b c h w -> b (h w) c')\n",
        "        k = rearrange(k, 'b c h w -> b c (h w)')\n",
        "        v = rearrange(v, 'b c h w -> b (h w) c')\n",
        "\n",
        "        z = F.softmax(q @ k, dim=-1) @ v\n",
        "        z = rearrange(z, 'b (h w) c -> b c h w', h=h, w=w)\n",
        "\n",
        "        return x + z"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff0de663-9f0d-4f66-96f5-ac73b8a50a5d",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "ff0de663-9f0d-4f66-96f5-ac73b8a50a5d"
      },
      "source": [
        "### 2.8. Downsample Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "58e94306-6722-4337-8439-dfd7c83237dc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T11:01:15.635383Z",
          "iopub.status.busy": "2023-12-05T11:01:15.634876Z",
          "iopub.status.idle": "2023-12-05T11:01:15.640432Z",
          "shell.execute_reply": "2023-12-05T11:01:15.639532Z",
          "shell.execute_reply.started": "2023-12-05T11:01:15.635341Z"
        },
        "tags": [],
        "id": "58e94306-6722-4337-8439-dfd7c83237dc"
      },
      "outputs": [],
      "source": [
        "class DownsampleBlock(nn.Module):\n",
        "    \"\"\"Downsample block.\"\"\"\n",
        "\n",
        "    def __init__(self, *, in_channels: int, out_channels: int) -> None:\n",
        "        \"\"\"Initialize the module.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.sequential = nn.Sequential(\n",
        "            Normalization(in_channels=in_channels),\n",
        "            DownsampleConvolution(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "            ),\n",
        "            Activation(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "\n",
        "        return self.sequential(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b18953cd-86ec-492f-b8b4-346a7e6bfdaf",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "b18953cd-86ec-492f-b8b4-346a7e6bfdaf"
      },
      "source": [
        "### 2.9. Upsample Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fb8cb9de-d2b0-4aa2-87bb-938bd82a6953",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T11:01:17.467205Z",
          "iopub.status.busy": "2023-12-05T11:01:17.466824Z",
          "iopub.status.idle": "2023-12-05T11:01:17.471996Z",
          "shell.execute_reply": "2023-12-05T11:01:17.471171Z",
          "shell.execute_reply.started": "2023-12-05T11:01:17.467179Z"
        },
        "tags": [],
        "id": "fb8cb9de-d2b0-4aa2-87bb-938bd82a6953"
      },
      "outputs": [],
      "source": [
        "class UpsampleBlock(nn.Module):\n",
        "    \"\"\"Upsample block.\"\"\"\n",
        "\n",
        "    def __init__(self, *, in_channels: int, out_channels: int) -> None:\n",
        "        \"\"\"Initialize the module.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.sequential = nn.Sequential(\n",
        "            Normalization(in_channels=in_channels),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            Convolution(in_channels=in_channels, out_channels=out_channels),\n",
        "            Activation(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "\n",
        "        return self.sequential(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a342bbc-851c-48fa-812a-a2a2b2c0da91",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "0a342bbc-851c-48fa-812a-a2a2b2c0da91"
      },
      "source": [
        "### 2.10. Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "60150754-7895-4b84-9a49-2f511cd5b75b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T11:03:19.033217Z",
          "iopub.status.busy": "2023-12-05T11:03:19.032832Z",
          "iopub.status.idle": "2023-12-05T11:03:19.040066Z",
          "shell.execute_reply": "2023-12-05T11:03:19.039281Z",
          "shell.execute_reply.started": "2023-12-05T11:03:19.033191Z"
        },
        "tags": [],
        "id": "60150754-7895-4b84-9a49-2f511cd5b75b"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        in_channels: int,\n",
        "        hidden_channels: int,\n",
        "        embedding_channels: int,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize the module.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.sequential = nn.Sequential(\n",
        "\n",
        "            # Downsampling.\n",
        "\n",
        "            Convolution(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=hidden_channels,\n",
        "            ),\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels),\n",
        "            DownsampleBlock(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "            ),\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels),\n",
        "            DownsampleBlock(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=hidden_channels*2,\n",
        "            ),\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels*2),\n",
        "            DownsampleBlock(\n",
        "                in_channels=hidden_channels*2,\n",
        "                out_channels=hidden_channels*2,\n",
        "            ),\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels*2),\n",
        "            DownsampleBlock(\n",
        "                in_channels=hidden_channels*2,\n",
        "                out_channels=hidden_channels*4,\n",
        "            ),\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels*4),\n",
        "\n",
        "            # Attention.\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels*4),\n",
        "            AttentionBlock(in_channels=hidden_channels*4),\n",
        "            ResNetBlock(in_channels=hidden_channels*4),\n",
        "\n",
        "            # Embedding.\n",
        "\n",
        "            Normalization(in_channels=hidden_channels*4),\n",
        "            Convolution(\n",
        "                in_channels=hidden_channels*4,\n",
        "                out_channels=embedding_channels,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "\n",
        "        return self.sequential(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd8b0c0-70d9-45ef-ad85-f402157f09da",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "6fd8b0c0-70d9-45ef-ad85-f402157f09da"
      },
      "source": [
        "### 2.11. Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a7bf491f-d31f-463a-9b42-249cbd393953",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T11:21:46.890248Z",
          "iopub.status.busy": "2023-12-05T11:21:46.889959Z",
          "iopub.status.idle": "2023-12-05T11:21:46.897809Z",
          "shell.execute_reply": "2023-12-05T11:21:46.897122Z",
          "shell.execute_reply.started": "2023-12-05T11:21:46.890224Z"
        },
        "tags": [],
        "id": "a7bf491f-d31f-463a-9b42-249cbd393953"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        out_channels: int,\n",
        "        hidden_channels: int,\n",
        "        embedding_channels: int,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize the module.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.sequential = nn.Sequential(\n",
        "\n",
        "            # Unembedding.\n",
        "\n",
        "            Convolution(\n",
        "                in_channels=embedding_channels,\n",
        "                out_channels=hidden_channels*4,\n",
        "            ),\n",
        "\n",
        "            # Attention.\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels*4),\n",
        "            AttentionBlock(in_channels=hidden_channels*4),\n",
        "            ResNetBlock(in_channels=hidden_channels*4),\n",
        "\n",
        "            # Upsampling.\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels*4),\n",
        "            UpsampleBlock(\n",
        "                in_channels=hidden_channels*4,\n",
        "                out_channels=hidden_channels*2,\n",
        "            ),\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels*2),\n",
        "            UpsampleBlock(\n",
        "                in_channels=hidden_channels*2,\n",
        "                out_channels=hidden_channels*2,\n",
        "            ),\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels*2),\n",
        "            UpsampleBlock(\n",
        "                in_channels=hidden_channels*2,\n",
        "                out_channels=hidden_channels,\n",
        "            ),\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels),\n",
        "            UpsampleBlock(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "            ),\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels),\n",
        "\n",
        "            Normalization(in_channels=hidden_channels),\n",
        "            Convolution(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=out_channels,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "\n",
        "        return self.sequential(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71ee085e-ab12-46f6-b507-09be56ec8c03",
      "metadata": {
        "tags": [],
        "id": "71ee085e-ab12-46f6-b507-09be56ec8c03"
      },
      "source": [
        "### 2.12. Vector Quantizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8ea9c892-af5f-4c25-9268-c3fb5ab1cd46",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T13:38:29.357093Z",
          "iopub.status.busy": "2023-12-05T13:38:29.356706Z",
          "iopub.status.idle": "2023-12-05T13:38:29.364103Z",
          "shell.execute_reply": "2023-12-05T13:38:29.363370Z",
          "shell.execute_reply.started": "2023-12-05T13:38:29.357066Z"
        },
        "tags": [],
        "id": "8ea9c892-af5f-4c25-9268-c3fb5ab1cd46"
      },
      "outputs": [],
      "source": [
        "class VectorQuantizer(nn.Module):\n",
        "    \"\"\"Vector quantizer.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        codebook_size: int,\n",
        "        embedding_channels: int,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize the module.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.codebook_size = codebook_size\n",
        "        self.embedding_dimension = embedding_channels\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=codebook_size,\n",
        "            embedding_dim=embedding_channels,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        x = rearrange(x, 'b c h w -> (b h w) c')\n",
        "\n",
        "        distance = torch.sum(x ** 2, dim=1, keepdim=True) \\\n",
        "            + torch.sum(self.embedding.weight ** 2, dim=1) \\\n",
        "            - 2*(x @ self.embedding.weight.T)\n",
        "\n",
        "        tokens = distance.argmin(dim=1).detach()\n",
        "\n",
        "        quantized = self.embedding(tokens)\n",
        "        codebook_loss = F.mse_loss(quantized, x.detach())\n",
        "        commitment_loss = F.mse_loss(x, quantized.detach())\n",
        "\n",
        "        quantized = x + (quantized - x).detach()\n",
        "        quantized = rearrange(quantized, '(b h w) c -> b c h w', h=H, w=W)\n",
        "        tokens = tokens.view(B, H, W)\n",
        "\n",
        "        return quantized, tokens, codebook_loss, commitment_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9addcef8-07f9-4552-99a7-39f1106cd686",
      "metadata": {
        "id": "9addcef8-07f9-4552-99a7-39f1106cd686"
      },
      "source": [
        "### 2.13. Gumbel-softmax Vector Quantizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "295d4be4-dc83-4cce-9b3e-c6e480da3bf0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T13:38:30.991060Z",
          "iopub.status.busy": "2023-12-05T13:38:30.990679Z",
          "iopub.status.idle": "2023-12-05T13:38:30.994504Z",
          "shell.execute_reply": "2023-12-05T13:38:30.993744Z",
          "shell.execute_reply.started": "2023-12-05T13:38:30.991034Z"
        },
        "tags": [],
        "id": "295d4be4-dc83-4cce-9b3e-c6e480da3bf0"
      },
      "outputs": [],
      "source": [
        "class GumbelSoftmaxVectorQuantizer(nn.Module):\n",
        "    ...\n",
        "\n",
        "    # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d855ed44-bc07-4a98-9b18-6c6eb5ddbeee",
      "metadata": {
        "id": "d855ed44-bc07-4a98-9b18-6c6eb5ddbeee"
      },
      "source": [
        "### 2.14. VQ-VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "75238b5c-b766-47bd-82bb-d15dd1deeca4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-05T13:38:32.228591Z",
          "iopub.status.busy": "2023-12-05T13:38:32.227856Z",
          "iopub.status.idle": "2023-12-05T13:38:32.235287Z",
          "shell.execute_reply": "2023-12-05T13:38:32.234573Z",
          "shell.execute_reply.started": "2023-12-05T13:38:32.228558Z"
        },
        "tags": [],
        "id": "75238b5c-b766-47bd-82bb-d15dd1deeca4"
      },
      "outputs": [],
      "source": [
        "class VQVAE(nn.Module):\n",
        "    \"\"\"VQ-VAE.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        in_channels: int,\n",
        "        hidden_channels: int,\n",
        "        embedding_channels: int,\n",
        "        codebook_size: int,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize the module.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            in_channels=in_channels,\n",
        "            hidden_channels=hidden_channels,\n",
        "            embedding_channels=embedding_channels,\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            out_channels=in_channels,\n",
        "            hidden_channels=hidden_channels,\n",
        "            embedding_channels=embedding_channels,\n",
        "        )\n",
        "\n",
        "        self.codebook = VectorQuantizer(\n",
        "            embedding_channels=embedding_channels,\n",
        "            codebook_size=codebook_size,\n",
        "        )\n",
        "\n",
        "    def encode(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "    ) -> Tuple[\n",
        "        torch.Tensor,\n",
        "        torch.Tensor,\n",
        "        torch.Tensor,\n",
        "        torch.Tensor,\n",
        "    ]:\n",
        "        \"\"\"Encode an example.\"\"\"\n",
        "\n",
        "        return self.codebook(self.encoder(x))\n",
        "\n",
        "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Decode an example.\"\"\"\n",
        "\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "    ) -> Tuple[\n",
        "        torch.Tensor,\n",
        "        torch.Tensor,\n",
        "        torch.Tensor,\n",
        "        torch.Tensor,\n",
        "        torch.Tensor,\n",
        "    ]:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "\n",
        "        quantized, tokens, codebook_loss, commitment_loss = self.encode(x)\n",
        "\n",
        "        x = self.decode(quantized)\n",
        "\n",
        "        return x, quantized, tokens, codebook_loss, commitment_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.15. Patch Discriminator"
      ],
      "metadata": {
        "id": "xYoFci-zxgG-"
      },
      "id": "xYoFci-zxgG-"
    },
    {
      "cell_type": "code",
      "source": [
        "def patchify(x: torch.Tensor, patch_size: int) -> torch.Tensor:\n",
        "    \"\"\"Patchify an image.\"\"\"\n",
        "\n",
        "    return rearrange(x, 'b c (h ph) (w pw) -> b (h w) c ph pw', ph=patch_size, pw=patch_size)"
      ],
      "metadata": {
        "id": "_tZDPp5AynL1"
      },
      "id": "_tZDPp5AynL1",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchDiscriminator(nn.Module):\n",
        "    \"\"\"Patch discriminator.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        in_channels: int,\n",
        "        hidden_channels: int,\n",
        "        patch_size: int,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize the module.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.sequential = nn.Sequential(\n",
        "\n",
        "            Convolution(in_channels=in_channels, out_channels=hidden_channels),\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels),\n",
        "            DownsampleBlock(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "            ),\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels),\n",
        "            DownsampleBlock(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=hidden_channels*2,\n",
        "            ),\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels*2),\n",
        "            DownsampleBlock(\n",
        "                in_channels=hidden_channels*2,\n",
        "                out_channels=hidden_channels*2,\n",
        "            ),\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels*2),\n",
        "            DownsampleBlock(\n",
        "                in_channels=hidden_channels*2,\n",
        "                out_channels=hidden_channels*4,\n",
        "            ),\n",
        "\n",
        "            ResNetBlock(in_channels=hidden_channels*4),\n",
        "            Normalization(in_channels=hidden_channels*4),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(out_features=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "\n",
        "        b, c, h, w = x.shape\n",
        "\n",
        "        x = patchify(x, patch_size=self.patch_size)\n",
        "        x = rearrange(x, 'b p c h w -> (b p) c h w')\n",
        "        x = self.sequential(x)\n",
        "        x = rearrange(x, '(b p) o -> b p o', b=b)\n",
        "        x = x.mean(dim=1)  # Average scores accross patches.\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "vzgck-29xi7k"
      },
      "id": "vzgck-29xi7k",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training"
      ],
      "metadata": {
        "id": "3N9rRlAqn4Bc"
      },
      "id": "3N9rRlAqn4Bc"
    },
    {
      "cell_type": "code",
      "source": [
        "resolution = 256\n",
        "channels = 1\n",
        "\n",
        "transform = Compose([\n",
        "    Resize((resolution, resolution), interpolation=0),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = MNIST(root='.', train=True, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "Vj-X9-MMoBWq"
      },
      "id": "Vj-X9-MMoBWq",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = VQVAE(in_channels=3, hidden_channels=128, embedding_channels=256, codebook_size=1024).cuda()\n",
        "\n",
        "device = 'cuda:0'\n",
        "\n",
        "generator = VQVAE(\n",
        "    in_channels=1,\n",
        "    hidden_channels=32,\n",
        "    embedding_channels=256,\n",
        "    codebook_size=32,\n",
        ").to(device)\n",
        "\n",
        "discriminator = PatchDiscriminator(\n",
        "    in_channels=1,\n",
        "    hidden_channels=32,\n",
        "    patch_size=32,\n",
        ").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW2A2A2So3Bb",
        "outputId": "193d4096-094d-4184-9631-8f418ff227b1"
      },
      "id": "mW2A2A2So3Bb",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = Adam(generator.parameters(), lr=1e-3)\n",
        "discriminator_optimizer = Adam(discriminator.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "S8PZRPoBn97d"
      },
      "id": "S8PZRPoBn97d",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./reconstruction-*.png"
      ],
      "metadata": {
        "id": "xblrv7mHuHrb"
      },
      "id": "xblrv7mHuHrb",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "epochs = 20\n",
        "batches = len(dataset) // batch_size\n",
        "accumulate_steps = 1\n",
        "\n",
        "generator_dataloader = DataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        ")\n",
        "\n",
        "discriminator_dataloader = (x for x in DataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "))\n",
        "\n",
        "beta = 0.25\n",
        "step = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch, data in enumerate(generator_dataloader):\n",
        "\n",
        "        x = data[0].view(-1, channels, resolution, resolution)\n",
        "        x = x.to(device)\n",
        "\n",
        "        # VQ-VAE training.\n",
        "\n",
        "        generator_optimizer.zero_grad()\n",
        "\n",
        "        generator_output, quantized, tokens, codebook_loss, commitment_loss = generator(x)\n",
        "        generator_output = F.sigmoid(generator_output)\n",
        "\n",
        "        perceptual_loss = F.binary_cross_entropy(generator_output, x)  # TODO: use an actual perceptual loss.\n",
        "        vq_vae_loss = perceptual_loss + codebook_loss + (beta * commitment_loss)\n",
        "        vq_vae_loss.backward()\n",
        "\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        # GAN training.\n",
        "\n",
        "        # Train the discriminator.\n",
        "\n",
        "        discriminator_optimizer.zero_grad()\n",
        "\n",
        "        fake_input = generator_output[: batch_size // 2].detach()  # Take first half of fake inputs and second half of real inputs.\n",
        "        real_input = x[batch_size // 2 :]\n",
        "\n",
        "        discriminator_input = torch.cat((fake_input, real_input))\n",
        "        discriminator_label = torch.cat((torch.zeros(batch_size // 2), torch.ones(batch_size // 2))).to(device)\n",
        "        discriminator_output = discriminator(discriminator_input).flatten()\n",
        "\n",
        "        discriminator_loss = F.binary_cross_entropy(discriminator_output, discriminator_label)\n",
        "        discriminator_loss.backward()\n",
        "\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        # Train the generator.\n",
        "\n",
        "        generator_optimizer.zero_grad()\n",
        "\n",
        "        generator_output, *_ = generator(x)\n",
        "\n",
        "        discriminator_input = generator_output[batch_size // 2 :]  # Take second half of fake inputs (not yet seen by the discriminator).\n",
        "        discriminator_label = torch.ones(batch_size // 2).to(device)\n",
        "        discriminator_output = discriminator(discriminator_input).flatten()\n",
        "\n",
        "        generator_loss = F.binary_cross_entropy(discriminator_output, discriminator_label)\n",
        "        generator_loss.backward()\n",
        "\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        if (step % 50) == 0:\n",
        "\n",
        "            vq_vae_loss = vq_vae_loss.detach().item()\n",
        "            discriminator_loss = discriminator_loss.detach().item()\n",
        "            generator_loss = generator_loss.detach().item()\n",
        "\n",
        "            print(f'epoch: {epoch:06d}/{epochs}, batch: {batch:06d}/{batches}, step: {step:06d} - vq-vae loss: {vq_vae_loss:0.3f}, discriminator loss: {discriminator_loss:0.3f}, generator loss: {generator_loss:0.3f}')\n",
        "\n",
        "            save_image(generator_output, f'./reconstruction-{step:06d}.png')\n",
        "\n",
        "        step += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "x68mdBKQpIPQ",
        "outputId": "25c81dd7-9d21-4dcc-9d9d-c04e1e228fc7"
      },
      "id": "x68mdBKQpIPQ",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 000000/20, batch: 000000/3750, step: 000000 - vq-vae loss: 1.358, discriminator loss: 0.133, generator loss: 0.691\n",
            "epoch: 000000/20, batch: 000050/3750, step: 000050 - vq-vae loss: 0.782, discriminator loss: 0.013, generator loss: 0.000\n",
            "epoch: 000000/20, batch: 000100/3750, step: 000100 - vq-vae loss: 0.581, discriminator loss: 0.005, generator loss: 0.000\n",
            "epoch: 000000/20, batch: 000150/3750, step: 000150 - vq-vae loss: 0.427, discriminator loss: 0.002, generator loss: 0.000\n",
            "epoch: 000000/20, batch: 000200/3750, step: 000200 - vq-vae loss: 0.589, discriminator loss: 0.006, generator loss: 0.005\n",
            "epoch: 000000/20, batch: 000250/3750, step: 000250 - vq-vae loss: 0.376, discriminator loss: 0.000, generator loss: 0.000\n",
            "epoch: 000000/20, batch: 000300/3750, step: 000300 - vq-vae loss: 0.361, discriminator loss: 0.027, generator loss: 0.002\n",
            "epoch: 000000/20, batch: 000350/3750, step: 000350 - vq-vae loss: 0.314, discriminator loss: 0.000, generator loss: 0.002\n",
            "epoch: 000000/20, batch: 000400/3750, step: 000400 - vq-vae loss: 0.272, discriminator loss: 0.002, generator loss: 0.008\n",
            "epoch: 000000/20, batch: 000450/3750, step: 000450 - vq-vae loss: 0.248, discriminator loss: 0.000, generator loss: 0.002\n",
            "epoch: 000000/20, batch: 000500/3750, step: 000500 - vq-vae loss: 0.230, discriminator loss: 0.000, generator loss: 0.004\n",
            "epoch: 000000/20, batch: 000550/3750, step: 000550 - vq-vae loss: 0.250, discriminator loss: 0.000, generator loss: 0.004\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c3077b8879bc>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mdiscriminator_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Take second half of fake inputs (not yet seen by the discriminator).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mdiscriminator_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mdiscriminator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3EF8MYshEYSB"
      },
      "id": "3EF8MYshEYSB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "e1f14331-b424-44b3-8056-61d9f0562a87",
        "c40c1961-50b3-4acb-b8b4-91b06b2a1c83",
        "6d94fd90-3d4d-47bd-b741-e4f438a13743",
        "7d72bf03-252b-43a6-8d2d-9407f3fd3667",
        "94333634-5c5a-466b-ba33-ebf64354225c",
        "d87ebc7a-43be-40dd-a3c9-d3d91f1556dc",
        "b626d3be-6600-4cb8-965b-7ef8639d315f",
        "0cf96674-f440-450f-ba45-a91855cb234b",
        "ff0de663-9f0d-4f66-96f5-ac73b8a50a5d",
        "b18953cd-86ec-492f-b8b4-346a7e6bfdaf",
        "0a342bbc-851c-48fa-812a-a2a2b2c0da91",
        "6fd8b0c0-70d9-45ef-ad85-f402157f09da",
        "71ee085e-ab12-46f6-b507-09be56ec8c03",
        "9addcef8-07f9-4552-99a7-39f1106cd686",
        "d855ed44-bc07-4a98-9b18-6c6eb5ddbeee",
        "xYoFci-zxgG-"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}