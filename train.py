# -*- coding: utf-8 -*-
"""VQ-GAN-FINAL-VERSION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DOLh3lsySNCS_LPHy7DJtJKfYghrWDx2

# VQ-GAN

## 1. Setup
"""

!pip -q install lightning einops datasets tokenizers

from typing import Tuple
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.utils.data import DataLoader

from torchvision.datasets import MNIST
from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage
from torchvision.utils import save_image

from torch.optim import Adam

import lightning as L

from einops import rearrange

from tqdm import tqdm

"""## 2. Model

### 2.1. Normalization
"""

Normalization = lambda in_channels: nn.GroupNorm(
    num_groups=32,
    num_channels=in_channels,
)

"""### 2.2. Convolution"""

Convolution = lambda in_channels, out_channels: nn.Conv2d(
    in_channels=in_channels,
    out_channels=out_channels,
    kernel_size=3,
    stride=1,
    padding=1,
)

"""### 2.3. Pointwise Convolution"""

PointwiseConvolution = lambda in_channels: nn.Conv2d(
    in_channels=in_channels,
    out_channels=in_channels,
    kernel_size=1,
    stride=1,
    padding=0,
)

"""### 2.4. Downsample Convolution"""

DownsampleConvolution = lambda in_channels, out_channels: nn.Conv2d(
    in_channels=in_channels,
    out_channels=out_channels,
    kernel_size=4,
    stride=2,
    padding=1,
)

"""### 2.5. Activation"""

Activation = lambda: nn.LeakyReLU(0.2)

"""### 2.6. ResNet Block"""

class ResNetBlock(nn.Module):
    """ResNet Block."""

    def __init__(self, *, in_channels: int) -> None:
        """Initialize the module."""

        super().__init__()

        self.sequential = nn.Sequential(
            Normalization(in_channels=in_channels),
            Activation(),
            Convolution(in_channels=in_channels, out_channels=in_channels),
            Normalization(in_channels=in_channels),
            Activation(),
            Convolution(in_channels=in_channels, out_channels=in_channels),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass."""

        return x + self.sequential(x)

"""### 2.7. Attention Block"""

class AttentionBlock(nn.Module):
    """Attention block."""

    def __init__(self, *, in_channels: int) -> None:
        """Initialize the module."""

        super().__init__()

        self.normalization = Normalization(in_channels=in_channels)
        self.project_q = PointwiseConvolution(in_channels=in_channels)
        self.project_k = PointwiseConvolution(in_channels=in_channels)
        self.project_v = PointwiseConvolution(in_channels=in_channels)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass."""

        b, c, h, w = x.shape
        z = self.normalization(x)

        q = self.project_q(z)
        k = self.project_k(z)
        v = self.project_v(z)

        q = rearrange(q, 'b c h w -> b (h w) c')
        k = rearrange(k, 'b c h w -> b c (h w)')
        v = rearrange(v, 'b c h w -> b (h w) c')

        z = F.softmax(q @ k, dim=-1) @ v
        z = rearrange(z, 'b (h w) c -> b c h w', h=h, w=w)

        return x + z

"""### 2.8. Downsample Block"""

class DownsampleBlock(nn.Module):
    """Downsample block."""

    def __init__(self, *, in_channels: int, out_channels: int) -> None:
        """Initialize the module."""

        super().__init__()

        self.sequential = nn.Sequential(
            Normalization(in_channels=in_channels),
            DownsampleConvolution(
                in_channels=in_channels,
                out_channels=out_channels,
            ),
            Activation(),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass."""

        return self.sequential(x)

"""### 2.9. Upsample Block"""

class UpsampleBlock(nn.Module):
    """Upsample block."""

    def __init__(self, *, in_channels: int, out_channels: int) -> None:
        """Initialize the module."""

        super().__init__()

        self.sequential = nn.Sequential(
            Normalization(in_channels=in_channels),
            nn.Upsample(scale_factor=2),
            Convolution(in_channels=in_channels, out_channels=out_channels),
            Activation(),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass."""

        return self.sequential(x)

"""### 2.10. Encoder"""

class Encoder(nn.Module):
    """Encoder."""

    def __init__(
        self,
        *,
        in_channels: int,
        hidden_channels: int,
        embedding_channels: int,
    ) -> None:
        """Initialize the module."""

        super().__init__()

        self.sequential = nn.Sequential(

            # Downsampling.

            Convolution(
                in_channels=in_channels,
                out_channels=hidden_channels,
            ),

            ResNetBlock(in_channels=hidden_channels),
            DownsampleBlock(
                in_channels=hidden_channels,
                out_channels=hidden_channels,
            ),

            ResNetBlock(in_channels=hidden_channels),
            DownsampleBlock(
                in_channels=hidden_channels,
                out_channels=hidden_channels*2,
            ),

            ResNetBlock(in_channels=hidden_channels*2),
            DownsampleBlock(
                in_channels=hidden_channels*2,
                out_channels=hidden_channels*2,
            ),

            ResNetBlock(in_channels=hidden_channels*2),
            DownsampleBlock(
                in_channels=hidden_channels*2,
                out_channels=hidden_channels*4,
            ),

            ResNetBlock(in_channels=hidden_channels*4),

            # Attention.

            ResNetBlock(in_channels=hidden_channels*4),
            AttentionBlock(in_channels=hidden_channels*4),
            ResNetBlock(in_channels=hidden_channels*4),

            # Embedding.

            Normalization(in_channels=hidden_channels*4),
            Convolution(
                in_channels=hidden_channels*4,
                out_channels=embedding_channels,
            ),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass."""

        return self.sequential(x)

"""### 2.11. Decoder"""

class Decoder(nn.Module):
    """Decoder."""

    def __init__(
        self,
        *,
        out_channels: int,
        hidden_channels: int,
        embedding_channels: int,
    ) -> None:
        """Initialize the module."""

        super().__init__()

        self.sequential = nn.Sequential(

            # Unembedding.

            Convolution(
                in_channels=embedding_channels,
                out_channels=hidden_channels*4,
            ),

            # Attention.

            ResNetBlock(in_channels=hidden_channels*4),
            AttentionBlock(in_channels=hidden_channels*4),
            ResNetBlock(in_channels=hidden_channels*4),

            # Upsampling.

            ResNetBlock(in_channels=hidden_channels*4),
            UpsampleBlock(
                in_channels=hidden_channels*4,
                out_channels=hidden_channels*2,
            ),

            ResNetBlock(in_channels=hidden_channels*2),
            UpsampleBlock(
                in_channels=hidden_channels*2,
                out_channels=hidden_channels*2,
            ),

            ResNetBlock(in_channels=hidden_channels*2),
            UpsampleBlock(
                in_channels=hidden_channels*2,
                out_channels=hidden_channels,
            ),

            ResNetBlock(in_channels=hidden_channels),
            UpsampleBlock(
                in_channels=hidden_channels,
                out_channels=hidden_channels,
            ),

            ResNetBlock(in_channels=hidden_channels),

            Normalization(in_channels=hidden_channels),
            Convolution(
                in_channels=hidden_channels,
                out_channels=out_channels,
            ),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass."""

        return self.sequential(x)

"""### 2.12. Vector Quantizer"""

class VectorQuantizer(nn.Module):
    """Vector quantizer."""

    def __init__(
        self,
        codebook_size: int,
        embedding_channels: int,
    ) -> None:
        """Initialize the module."""

        super().__init__()

        self.codebook_size = codebook_size
        self.embedding_dimension = embedding_channels

        self.embedding = nn.Embedding(
            num_embeddings=codebook_size,
            embedding_dim=embedding_channels,
        )

    def forward(
        self,
        x: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """Forward pass."""

        B, C, H, W = x.shape
        x = rearrange(x, 'b c h w -> (b h w) c')

        distance = torch.sum(x ** 2, dim=1, keepdim=True) \
            + torch.sum(self.embedding.weight ** 2, dim=1) \
            - 2*(x @ self.embedding.weight.T)

        tokens = distance.argmin(dim=1).detach()

        quantized = self.embedding(tokens)
        codebook_loss = F.mse_loss(quantized, x.detach())
        commitment_loss = F.mse_loss(x, quantized.detach())

        quantized = x + (quantized - x).detach()
        quantized = rearrange(quantized, '(b h w) c -> b c h w', h=H, w=W)
        tokens = tokens.view(B, H, W)

        return quantized, tokens, codebook_loss, commitment_loss

"""### 2.13. Gumbel-softmax Vector Quantizer"""

class GumbelSoftmaxVectorQuantizer(nn.Module):
    ...

    # TODO

"""### 2.14. VQ-VAE"""

class VQVAE(nn.Module):
    """VQ-VAE."""

    def __init__(
        self,
        *,
        in_channels: int,
        hidden_channels: int,
        embedding_channels: int,
        codebook_size: int,
    ) -> None:
        """Initialize the module."""

        super().__init__()

        self.encoder = Encoder(
            in_channels=in_channels,
            hidden_channels=hidden_channels,
            embedding_channels=embedding_channels,
        )

        self.decoder = Decoder(
            out_channels=in_channels,
            hidden_channels=hidden_channels,
            embedding_channels=embedding_channels,
        )

        self.codebook = VectorQuantizer(
            embedding_channels=embedding_channels,
            codebook_size=codebook_size,
        )

    def encode(
        self,
        x: torch.Tensor,
    ) -> Tuple[
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
    ]:
        """Encode an example."""

        return self.codebook(self.encoder(x))

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        """Decode an example."""

        return self.decoder(z)

    def forward(
        self,
        x: torch.Tensor,
    ) -> Tuple[
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
    ]:
        """Forward pass."""

        quantized, tokens, codebook_loss, commitment_loss = self.encode(x)

        x = self.decode(quantized)

        return x, quantized, tokens, codebook_loss, commitment_loss

"""## 3. Training"""

resolution = 256

transform = Compose([
    Resize((resolution, resolution), interpolation=0),
    ToTensor(),
])

dataset = MNIST(root='.', train=True, download=True, transform=transform)

#model = VQVAE(in_channels=3, hidden_channels=128, embedding_channels=256, codebook_size=1024).cuda()

device = 'cuda:0'

model = VQVAE(
    in_channels=1,
    hidden_channels=32,
    embedding_channels=256,
    codebook_size=32,
).to(device)

print(f'model size: {sum([parameter.numel() for parameter in model.parameters()])/1e6:0.2f}M parameters.')

optimizer = Adam(model.parameters(), lr=1e-3)

!rm -rf ./reconstruction-*.png

batch_size = 16
epochs = 20
batches = len(dataset) // batch_size
accumulate_steps = 1

dataloader = DataLoader(
    dataset=dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=2,
)

beta = 0.25
step = 0

for epoch in range(epochs):
    for batch, data in enumerate(dataloader):

        x = data[0].view(-1, 1, resolution, resolution)
        x = x.to(device)

        optimizer.zero_grad()

        output, quantized, tokens, codebook_loss, commitment_loss = model(x)
        output = F.sigmoid(output)

        reconstruction_loss = F.binary_cross_entropy(output, x)
        loss = reconstruction_loss + codebook_loss + (beta * commitment_loss)
        loss.backward()

        if (step % accumulate_steps) == 0:
            optimizer.step()
            step += 1

        if (step % 100) == 0:
            loss_1 = loss.detach().item()
            loss_2 = reconstruction_loss.detach().item()
            loss_3 = codebook_loss.detach().item()

            print(f'epoch: {epoch:06d}/{epochs}, batch: {batch:06d}/{batches}, step: {step:06d} - loss: {loss_1:0.3f}, reconstruction loss: {loss_2:0.3f}, codebook loss: {loss_3:0.3f}')

            save_image(output, f'./reconstruction-{step:06d}.png')

